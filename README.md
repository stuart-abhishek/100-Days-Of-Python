---

ğŸ 100 Days of Python â€” by Stuart Abhishek

> â€œSmall consistent steps create giant success.â€ â€” Stuart Abhishek




---

ğŸŒ About This Repository

Welcome to my 100 Days of Python challenge!

This repository documents my 100-day journey to master Python â€” one project every day, combining creativity, logic, and real-world problem-solving.
Each project is designed with clean structure, interactive design, and modern programming concepts that reflect the skills of a future Computer Science Engineering at â†’ MIT â†’ Stanford.


---

ğŸ¯ My Vision

ğŸ”¥ Master Python â€” from fundamentals to advanced algorithms.

ğŸ’¡ Think like an engineer, design like an artist, and build like a scientist.

ğŸš€ Develop projects that prove consistency, logic, and innovation.

ğŸ“ Achieve admission into top universities â€” MIT, Stanford â€” through skills and passion.



---

ğŸ§© Progress Tracker

Day	Project	Description	Status

1 	Hello World	My first Python program â€” start of my journey	âœ…
2	 AI Quote Generator	Personalized motivational quote generator	âœ…
3	 Smart Math Quiz	Adaptive arithmetic quiz with scoring & logic	âœ…
4 	Secure Password Engineer	Cryptographically secure password generator + analyzer	âœ…
5 Natural language smart calculator âœ…
6 Smart Data Analyzer âœ…
7 Predictive Insight Engine âœ…
8 From Scratch Naive Bayes Text Classifier âœ…
9 Clustering Insight Engine âœ…
10 Sudoku Engineer âœ…
11 MiniGit: a Content-Addressable Version Store âœ…
12 Distributed MiniGit âœ…
13 CRDT Notes âœ…
14 CRDT Collaborative Editor âœ…
15 Neural Network From Scratch + Visualizer âœ…


---

ğŸ“˜ Project Details


---

ğŸ§  Day 1 â€” Hello World ğŸ‘‹

ğŸ”¹ Project Title

Hello World Program â€” My first step into the world of programming.

ğŸ”¹ Description

A simple Python script that prints a motivational message.
This marks the beginning of my 100-day journey â€” the foundation of everything that follows.

ğŸ”¹ Code

# Day 1 â€“ Hello World Program
# Author: Stuart Abhishek

print("Hello, World! This is Day 1 of my 100 Days of Python challenge.")

ğŸ”¹ Example Output

Hello, World! This is Day 1 of my 100 Days of Python challenge.

ğŸ”¹ What I Learned

Basic Python syntax

Printing output to the console

My first taste of programming discipline ğŸ’ª



---

ğŸ§  Day 2 â€” AI-Style Quote Generator ğŸ¤–

ğŸ”¹ Project Title

AI Quote Generator â€” A personalized AI-like motivational quote system.

ğŸ”¹ Description

This Python script interacts with the user to create motivational quotes based on their name and goal.
It uses randomness and time-based logic to create human-like responses â€” blending creativity with computation.

ğŸ”¹ Code

# Day 2 â€“ AI-Style Quote Generator
# Author: Stuart Abhishek

import random, datetime

print("ğŸ¤– Welcome to the AI Quote Generator!")
print("Let's create a personalized quote to inspire you today.\n")

name = input("What is your name? ")
goal = input("Whatâ€™s one goal youâ€™re working on right now? ")

quotes = [
  f"{name}, remember â€” every expert was once a beginner. Keep pushing toward {goal}!",
  f"Success doesnâ€™t come from what you do occasionally, {name}, it comes from what you do consistently for {goal}.",
  f"{name}, when you feel like quitting, think about why you started {goal}.",
  f"The future belongs to those like {name} who never stop learning while chasing {goal}.",
  f"{name}, small steps every day towards {goal} will lead to massive results."
]

quote = random.choice(quotes)
hour = datetime.datetime.now().hour
greeting = "Good morning" if hour < 12 else "Good afternoon" if hour < 18 else "Good evening"

print("\n" + "="*60)
print(f"{greeting}, {name}! ğŸŒŸ")
print("Hereâ€™s your motivational message:")
print(f"ğŸ’¬  {quote}")
print("="*60)
print("~ Program created by Stuart Abhishek (Day 2 of 100 Days of Python) ~")

ğŸ”¹ Example Output

ğŸ¤– Welcome to the AI Quote Generator!
Let's create a personalized quote to inspire you today.

What is your name? Stuart
Whatâ€™s one goal youâ€™re working on right now? Python mastery

============================================================
Good evening, Stuart! ğŸŒŸ
Hereâ€™s your motivational message:
ğŸ’¬  Stuart, when you feel like quitting, think about why you started Python mastery.
============================================================
~ Program created by Stuart Abhishek (Day 2 of 100 Days of Python) ~

ğŸ”¹ Concepts Used

Variables & Input

Randomization

datetime module

String formatting

Conditional statements


ğŸ”¹ What I Learned

How to make interactive programs

Personalization through logic

Creating â€œhuman-feelingâ€ code ğŸ¤–



---

ğŸ§  Day 3 â€” Smart Math Quiz ğŸ¯

ğŸ”¹ Project Title

Smart Math Quiz â€” Adaptive arithmetic quiz with scoring & difficulty levels.

ğŸ”¹ Description

This quiz challenges users with math problems that automatically increase in difficulty as you score higher.
It rewards accuracy, penalizes errors, and gives a professional performance summary â€” simulating a smart learning system.

ğŸ”¹ Code

# Day 3 â€“ Smart Math Quiz with Scoring System
# Author: Stuart Abhishek

import random, time

def generate_question(level):
  if level == 1:
    a, b = random.randint(1, 10), random.randint(1, 10)
    op = random.choice(['+', '-'])
  elif level == 2:
    a, b = random.randint(10, 50), random.randint(1, 20)
    op = random.choice(['+', '-', '*'])
  else:
    a, b = random.randint(20, 100), random.randint(1, 25)
    op = random.choice(['+', '-', '*', '//'])
  question = f"{a} {op} {b}"
  return question, eval(question)

def math_quiz():
  print("ğŸ§® Welcome to the Smart Math Quiz!")
  print("Answer as many questions as you can. Type 'quit' to stop.\n")

  level = 1; score = 0; count = 0; start = time.time()
  while True:
    count += 1
    q, ans = generate_question(level)
    user = input(f"Q{count}: {q} = ")
    if user.lower() == "quit": break
    try:
      if int(user) == ans:
        score += 10
        print("âœ… Correct!")
        if score % 50 == 0:
          level = min(level + 1, 3)
          print("ğŸš€ Level Up! Difficulty increased.")
      else:
        score -= 5
        print(f"âŒ Wrong! Correct answer was {ans}.")
    except ValueError:
      print("âš ï¸ Enter a number or 'quit'.")
    print(f"Current Score: {score}\n")

  t = round(time.time() - start, 2)
  print("="*55)
  print("ğŸ Quiz Summary")
  print(f"Questions: {count - 1} | Final Score: {score} | Time: {t}s")
  if score >= 100: print("ğŸŒŸ Brilliant work!")
  elif score >= 50: print("ğŸ’ª Great job!")
  else: print("ğŸ“˜ Keep practicing!")
  print("="*55)
  print("~ Program created by Stuart Abhishek (Day 3 of 100 Days of Python) ~")

if __name__ == "__main__":
  math_quiz()

ğŸ”¹ Example Output

Q1: 3 + 4 = 7
âœ… Correct!
Current Score: 10

Q2: 12 - 8 = 4
âœ… Correct!
Current Score: 20

Q3: 6 * 5 = 31
âŒ Wrong! Correct answer was 30.
Current Score: 15

ğŸ”¹ Concepts Used

Functions & Modular Design

Loops & Conditionals

Randomization & Adaptive Logic

Scoring Systems

Time Measurement


ğŸ”¹ What I Learned

Structured function design

Adaptive algorithms

Logic building like a real engineer âš™ï¸



---

ğŸ§  Day 4 â€” Secure Password Engineer ğŸ”

ğŸ”¹ Project Title

Secure Password Engineer â€” Cryptographically secure password generator + strength analyzer.

ğŸ”¹ Description

This professional-grade program creates uncrackable passwords and analyzes their strength using:

Entropy calculations

Pattern detection

Sequential run identification

Ambiguity checks

Comprehensive 0â€“100 scoring system


It uses the secrets module (cryptographically secure RNG) and outputs suggestions for improvement â€” just like a mini cybersecurity assistant.

ğŸ”¹ Code Highlights

secrets module for secure randomness

math.log2() for entropy estimation

Regular expressions for pattern detection

Logging system using pathlib

Modular functions and clean CLI


ğŸ”¹ Example Interaction

ğŸ” Secure Password Engineer â€” Day 4
1) Generate a strong password
2) Analyze an existing password
3) Generate & analyze (recommended)
q) Quit
Choose an option: 3
Desired length (recommend 16â€“24): 18
Include lowercase? [Y/n]:
Include uppercase? [Y/n]:
Include digits? [Y/n]:
Include symbols? [Y/n]:
Avoid ambiguous characters? [Y/n]:

Generated password:
7uG}xVbR%pZt_fH3q*

Score: 93/100 | Grade: Very Strong
Length: 18 | Entropy: 113.61 bits
Longest sequential run: 1
Repeated runs: False | Common pattern: False

Suggestions:
â€¢ Great length â€” keep 16+ for stronger security.
â€¢ Excellent character diversity â€” 4 categories used.

ğŸ”¹ Concepts Used

Cryptography & Secure Randomness

Entropy and Information Theory

Pattern Recognition

Data Validation

File Logging and Modular Programming


ğŸ”¹ What I Learned

Difference between random and secrets

How to measure password entropy mathematically

Writing security-conscious, user-friendly code

Designing an AI-style analytical program


---

## ğŸ§  Day 5 â€” Natural-Language Smart Calculator ğŸ¤–

### ğŸ”¹ Project Title
**Smart Calculator** â€” Understands human language to perform arithmetic.

### ğŸ”¹ Project Description
A Python program that interprets natural-language expressions like  
â€œadd 12 and 45â€, â€œsubtract 10 from 50â€, â€œsquare root of 81â€,  
and computes accurate results.  
It mimics early natural-language interfaces â€” showing algorithmic reasoning and text processing skills.

### ğŸ”¹ Concepts Used
- Regular Expressions (`re`) for text extraction  
- Conditional Logic for intent detection  
- Mathematical operations (`math` module)  
- Exception handling  
- Clean modular programming  

### ğŸ”¹ Example Output

ğŸ§® Welcome to the Natural-Language Smart Calculator! ğŸ‘‰ Enter expression: add 24 and 65 âœ… Result: 89.0

ğŸ‘‰ Enter expression: subtract 10 from 42 âœ… Result: 32.0

ğŸ‘‰ Enter expression: square root of 81 âœ… Result: 9.0

ğŸ‘‰ Enter expression: cube of 3 âœ… Result: 27.0

### ğŸ”¹ What I Learned
- Translating language into computation  
- Regex pattern matching and token parsing  
- Handling ambiguous input gracefully  
- Thinking algorithmically like a language-model designer  

### ğŸ”¹ Future Improvements
- Integrate `nltk` or `spaCy` for deeper natural-language parsing  
- Add unit conversion and scientific-mode operations

---

## ğŸ§  Day 6 â€” Smart Data Analyzer ğŸ“Š

### ğŸ”¹ Project Title
**Smart Data Analyzer** â€” automatic statistical and correlation analysis of CSV datasets.

### ğŸ”¹ Project Description
This Python engine loads any CSV file and instantly produces summary statistics, detects strong correlations, and provides simple â€œinsights.â€  
It demonstrates data-science fundamentals, algorithmic thinking, and data-driven storytelling.

### ğŸ”¹ Concepts Used
- File I/O & CSV parsing (`csv.DictReader`)
- Statistics & probability (`statistics`, `math`)
- Correlation coefficient computation
- Data visualization with `matplotlib`
- Algorithmic automation & reporting

### ğŸ”¹ Example Output

ğŸ“Š Smart Data Analyzer â€” Day 6 Enter CSV file path (e.g., data.csv): students.csv

ğŸ“ˆ Summary Statistics â€¢ Math: mean=78.4, median=80.0, stdev=10.2, n=50 â€¢ Science: mean=76.1, median=75.0, stdev=9.5, n=50 â€¢ English: mean=81.6, median=82.0, stdev=8.9, n=50

ğŸ¤ Significant Correlations (|r| â‰¥ 0.5) Math â†” Science: r = 0.91 (direct correlation) English â†” Math: r = 0.73 (direct correlation)

âœ¨ Insights: Strongest link: Math and Science (0.91). Consider exploring cause-effect relationship. Report complete âœ…

### ğŸ”¹ What I Learned
- Reading structured data programmatically  
- Statistical reasoning (mean, median, stdev, correlation)  
- Automating analysis workflows like real data scientists  
- Presenting information visually and narratively  

### ğŸ”¹ Future Improvements
- Integrate `pandas` for larger datasets  
- Export summary reports as PDF  
- Apply linear regression to predict relationships  
- Build a web dashboard using `Streamlit`

---
  
## ğŸ§  Day 7 â€” Predictive Insight Engine ğŸ“ˆ

### ğŸ”¹ Project Title
**Predictive Insight Engine** â€” Univariate Linear Regression with Cross-Validation, Outlier Handling, Plots, and a Model Card.

### ğŸ”¹ Project Description
A disciplined mini-ML pipeline:
- Loads a CSV, selects a numeric feature (X) and target (Y)
- Optional z-score outlier removal
- Standardizes features/targets
- Trains linear regression via gradient descent with early stopping
- Reports **RÂ², MAE, RMSE** and performs **5-fold cross-validation**
- Shows **fitted line** and **residual diagnostics** plots
- Exports a **JSON model card** (coefficients, scalers, CV metrics, metadata)

### ğŸ”¹ Concepts Used
- Data hygiene (z-scores), standardization
- Gradient descent & early stopping
- Generalization via cross-validation
- Multiple evaluation metrics (RÂ², MAE, RMSE)
- Residual analysis and visualization
- Reproducibility (JSON model card)

### ğŸ”¹ Example Session

ğŸ“ˆ Predictive Insight Engine â€” Day 7 Enter CSV path (e.g., data.csv): students.csv Choose FEATURE (X):

1. StudyHours


2. SleepHours


3. MathScore Select number: 1 Choose TARGET (Y):


4. MathScore


5. ScienceScore Select number: 1 Remove outliers with z-score > 3? [Y/n]: Y Outlier filter: 52 â†’ 50 usable pairs.



ğŸ” 5-fold Cross-Validation R2_mean: 0.8123 R2_std: 0.0431 MAE_mean: 3.215 RMSE_mean: 4.097

âœ… Fitted on full data RÂ²: 0.8467 | MAE: 2.98 | RMSE: 3.82

Show fitted-line plot? [Y/n]: Y Show residuals plot? [Y/n]: Y Save model card JSON? [Y/n]: Y ğŸ“ Model card saved to: Day-07/model_card_StudyHours_to_MathScore.json

### ğŸ”¹ What I Learned
- How to build a small but **serious** ML workflow from scratch  
- Why **cross-validation** matters for generalization  
- Reading models beyond a single score using residuals  
- The importance of **reproducibility** through a model card

### ğŸ”¹ Future Improvements
- Multi-feature regression (normal equations)
- Polynomial basis expansion with regularization
- Confidence intervals and prediction intervals
- Export plots + report as a single HTML/PDF


---

## ğŸ§  Day 8 â€” From-Scratch Naive Bayes Text Classifier ğŸ“¨

### ğŸ”¹ Project Title
**Naive Bayes Text Classifier** â€” pure-Python NLP classifier with CV, explainability, and a model card.

### ğŸ”¹ Project Description
A full, explainable NLP pipeline implemented **from scratch**:
- Tokenizes text (stopword filtering)
- Trains a **Multinomial Naive Bayes** with **Laplace smoothing**
- Performs **5-fold cross-validation** (macro precision/recall/F1, accuracy)
- Prints a **confusion matrix**
- Shows **most-informative tokens** via class log-odds
- Exports a **JSON model card** (priors, vocab size, CV metrics, metadata)
- Includes an **interactive demo** for live classification

**Input format:** CSV with columns: `text`, `label`.

### ğŸ”¹ Concepts Used
- Probabilistic modeling (Naive Bayes)
- Tokenization, stopwords, <UNK> handling
- Cross-validation for generalization
- Macro-averaged **precision/recall/F1**
- Explainability (log-odds indicative tokens)
- Reproducibility (JSON model card)

### ğŸ”¹ Example Session

ğŸ§  Day 8 â€” Naive Bayes Text Classifier (From Scratch) Enter CSV path (must include columns 'text','label'): sms_spam.csv Laplace smoothing alpha [default 1.0]: Keep numeric tokens? [y/N]: y

ğŸ” 5-fold Cross-Validation (macro-averaged): accuracy: 0.962 precision_macro: 0.955 recall_macro: 0.948 f1_macro: 0.951

âœ… Fit on full data (reference metrics): accuracy: 0.971 precision_macro: 0.966 recall_macro: 0.959 f1_macro: 0.962

Confusion Matrix (rows=true, cols=pred): ham   spam ham    480     12 spam      5     73

ğŸ’¡ Most-informative tokens (log-odds): ham vs spam: meeting(2.31), home(2.07), okay(1.98), call(1.72), ... spam vs ham: free(3.45), prize(3.23), claim(3.10), txt(2.88), win(2.76), ... ğŸ“ Model card saved to: Day-08/model_card_naive_bayes.json text> win a free prize now! Predicted: spam  |  Probabilities: {'ham': 0.013, 'spam': 0.987}

### ğŸ”¹ What I Learned
- Implementing a classic ML algorithm from first principles
- Measuring generalization with CV (not just train accuracy)
- Reading models via **most-informative features**
- Building explainable, documented ML pipelines

### ğŸ”¹ Future Improvements
- Add TF-IDF weighting
- Character-level n-grams for robustness
- ROC-AUC, PR-AUC plots
- Save/load trained model for reuse


---

## ğŸ§  Day 9 â€” Clustering Insight Engine ğŸŒ

### ğŸ”¹ Project Title
**Clustering Insight Engine** â€” K-Means + PCA implemented from scratch with visualization and a model card.

### ğŸ”¹ Project Description
An unsupervised-learning engine that groups data into clusters, projects them via PCA, and visualizes patterns.  
It measures convergence, inertia, and exports a reproducible JSON summary.  
Demonstrates linear algebra, optimization, and data visualization fundamentals.

### ğŸ”¹ Concepts Used
- K-Means clustering (centroid update, inertia)
- PCA (eigenvectors via power iteration)
- Z-score normalization
- Algorithm convergence & tolerance
- Data visualization (`matplotlib`)
- Reproducibility via model card

### ğŸ”¹ Example Output

ğŸŒ Day 9 â€” Clustering Insight Engine Enter CSV path: iris_numeric.csv Number of clusters (k): 3

âœ… K-Means finished in 12 iterations. Inertia: 48.72 Cluster sizes: [52, 50, 48] ğŸ“ Saved Day-09/model_card_kmeans.json

*(2-D scatter plot of clusters displayed)*

### ğŸ”¹ What I Learned
- Implementing iterative optimization algorithms (K-Means)
- Reducing high-dimensional data with PCA
- Visualizing and interpreting unsupervised results
- Writing clear, reusable scientific code

### ğŸ”¹ Future Improvements
- Add **Elbow method** for automatic k selection  
- Implement **Silhouette score**  
- Extend PCA to N components  
- Build a simple GUI for cluster exploration


---

## ğŸ§  Day 10 â€” Sudoku Engineer ğŸ§©

### ğŸ”¹ Project Title
**Sudoku Engineer** â€” Generator + Solver via Exact Cover (Algorithm X), with Uniqueness Check and Difficulty Rating.

### ğŸ”¹ Project Description
A rigorous CS approach to Sudoku:
- Models Sudoku as an **Exact Cover** problem over 324 constraints
- Solves using **Algorithm X (Knuth)** with a minimum-remaining-values heuristic
- **Generates** new puzzles by carving a completed grid while preserving **unique** solutions
- Rates difficulty using search statistics (nodes explored, backtracks)
- Clean CLI to **solve from string/file** or **generate by clues**

### ğŸ”¹ Concepts Used
- Constraint modeling & combinatorial search
- Exact Cover reduction; Algorithm X backtracking
- Heuristics (MRV), uniqueness testing (multi-solution cutoff)
- Randomized full-grid construction, symmetric clue removal
- Software engineering: modularity, CLI, stats-based difficulty

### ğŸ”¹ Example Session

ğŸ§© Day 10 â€” Sudoku Engineer (Algorithm X)

1. Solve from 81-char string


2. Solve from text file


3. Generate puzzle (unique) by target clues q) Quit Choose: 3 Target number of clues (17..40)? [default 28]:



ğŸ§© Generated puzzle: . . . | 2 . . | . 1 . . 6 . | . . 3 | . . 7 9 . . | . . . | . . . ------+-------+------ . . . | . 7 . | 4 . . . . 1 | 3 . 6 | 7 . . . . . | . . . | . . . ------+-------+------ . . . | . . . | . . 8 8 . . | 6 . . | . 2 . . 2 . | . . 1 | . . .

Clues: 28 | Rating: Medium Generator effort â€” Nodes: 1523 | Backtracks: 214 | Time: 1.46s

Tip: Copy puzzle as 81 chars (row-major, '.' for blanks): ...2.. .1..6...3..7 9........ ....7.4.. ..13.67.. ......... .......8 8..6.. .2...1...

### ğŸ”¹ What I Learned
- How to **reduce** a real puzzle to an **exact cover** formulation
- Implementing a classic **Algorithm X** solver cleanly
- Designing a **generator** that guarantees **unique** solutions
- Interpreting search stats as a **difficulty rating**

### ğŸ”¹ Future Improvements
- Dancing Links (DLX) for faster exact cover operations
- Human-solvable strategy grader (naked pairs, X-wing, etc.)
- Export to PDF/PNG with pretty render
- Benchmark suite & seedable generation


---

## ğŸ§  Day 11 â€” MiniGit ğŸ—ƒï¸

### ğŸ”¹ Project Title
**MiniGit** â€” a lightweight, content-addressable version store built from scratch.

### ğŸ”¹ Project Description
A tiny re-implementation of Gitâ€™s core ideas:
- Stores files by **SHA-256 hash** (content-addressable)
- Tracks full directory snapshots as **commits** (DAG structure)
- Maintains **HEAD**, **commit logs**, and **checkout**
- Detects added/modified/removed files (`status` command)
- JSON metadata, deterministic object IDs, fully local

### ğŸ”¹ Concepts Used
- Cryptographic hashing (`hashlib.sha256`)
- Persistent object storage
- Directed acyclic graph (commit parent chain)
- File system traversal, diff logic
- CLI argument parsing & structured persistence (`json`)

### ğŸ”¹ Example Usage

$ python day11_minigit.py init âœ… Initialized empty MiniGit repository in ./.minigit

$ echo "Hello MIT" > hello.txt $ python day11_minigit.py commit "First commit" âœ… Commit created 3fae1b2c - First commit

$ echo "Hello Stanford" > hello.txt $ python day11_minigit.py status ğŸ“Š Status vs last commit: Modified: hello.txt

$ python day11_minigit.py commit "Updated greeting" âœ… Commit created 7bfc92ad - Updated greeting

$ python day11_minigit.py log ğŸ•’ Thu Nov 6 20:15:41 2025 ğŸ§© 7bfc92ad... ğŸ’¬ Updated greeting ğŸ•’ Thu Nov 6 20:14:12 2025 ğŸ§© 3fae1b2c... ğŸ’¬ First commit

$ python day11_minigit.py checkout 3fae1b2c âœ… Checked out commit 3fae1b2c

### ğŸ”¹ What I Learned
- How **Gitâ€™s architecture** works internally  
- Designing content-addressable storage (immutable objects)  
- Building a real-world **DAG data structure**  
- Understanding version control as a graph problem  

### ğŸ”¹ Future Improvements
- Branching and merging
- Commit diff viewer
- Blob compression
- Remote push/pull simulation


---

## ğŸ§  Day 12 â€” Distributed MiniGit ğŸŒ

### ğŸ”¹ Project Title
**Distributed MiniGit** â€” Peer-to-peer push/pull over TCP with content-addressable integrity.

### ğŸ”¹ Project Description
Extends my MiniGit (Day 11) into a tiny **distributed** VCS:
- Threaded TCP **server** announces inventory (HEAD, commits, objects)
- **Client push/pull** performs set reconciliation (what am I missing?)
- Transfers **commits/objects** as JSON-framed messages + raw blobs
- Verifies **SHA-256** on receive before storing (integrity)
- Fast-forwards **HEAD** when appropriate (safe sync)

### ğŸ”¹ Concepts Used
- Socket programming (TCP), concurrency (thread-per-connection)
- Wire protocol design (newline-delimited JSON frames)
- Content-addressable replication & integrity checks
- Inventory diff & reconciliation
- Clean layering on top of a local object store

### ğŸ”¹ Example Sessions

Terminal A (server)

$ python Day-12/day12_distributed_minigit.py serve --port 9999 ğŸŒ MiniGit server listening on 0.0.0.0:9999

Terminal B (client in another repo copy)

$ python Day-12/day12_distributed_minigit.py pull --host 127.0.0.1 --port 9999 â¬‡ï¸  Pulling commits: 2, objects: 5 ğŸ” Fast-forwarded HEAD to 3fae1b2c âœ… Pull complete.

Push back

$ python Day-12/day12_distributed_minigit.py push --host 127.0.0.1 --port 9999 â¡ï¸  Pushing commits: 1, objects: 1 âœ… Push complete.

### ğŸ”¹ What I Learned
- Designing a **minimal, auditable network protocol**
- Preventing corruption via **content-hash verification**
- Reconciling distributed state safely (fast-forward HEAD)
- Building distributed features on top of a local **content store**

### ğŸ”¹ Future Improvements
- Branch refs & non-fast-forward safety checks
- Packfiles / compression, chunked streaming
- Auth (HMAC or public-key signatures)
- Delta transmission & bloom filters for faster discovery


---

## ğŸ§  Day 13 â€” CRDT Notes ğŸ“

### ğŸ”¹ Project Title
**CRDT Notes** â€” Offline-first collaborative notes with LWW-Registers, Lamport clocks, tombstones, and peer sync.

### ğŸ”¹ Project Description
A conflict-free replicated data type (CRDT) system for notes:
- Each note field (title, body, deleted) is a **Last-Writer-Wins Register** with a **Lamport timestamp** â†’ deterministic conflict resolution without wall-clock time.
- **Tombstones** represent deletions (merge-safe).
- **Idempotent, commutative, associative merge** guarantees eventual consistency.
- Tiny **TCP sync**: peers exchange state JSON and both store the **same merged state**.

### ğŸ”¹ Concepts Used
- CRDT design (LWW-Register), tombstones
- **Lamport clock** for causality without real-time clocks
- Deterministic conflict resolution `(ts, node)` ordering
- Idempotent, associative, commutative **merge()**
- Minimal wire protocol; offline â†’ online sync convergence

### ğŸ”¹ Example Sessions

Replica A

$ python Day-13/day13_crdt_notes.py init --node-id A $ python Day-13/day13_crdt_notes.py new "Groceries" "Milk, eggs, bread" ğŸ†• Created note 6a7d-...

Replica B

$ python Day-13/day13_crdt_notes.py init --node-id B $ python Day-13/day13_crdt_notes.py new "Groceries" "Milk, eggs, bread, apples"

Now B edits title offline, A deletes offline:

$ python Day-13/day13_crdt_notes.py edit 6a7d-... --title "Shopping" $ python Day-13/day13_crdt_notes.py delete 6a7d-...

Sync (run server on A)

$ python Day-13/day13_crdt_notes.py serve --port 9500  # on A $ python Day-13/day13_crdt_notes.py sync --host 127.0.0.1 --port 9500  # on B

ğŸ” Sync complete. States converged.

### ğŸ”¹ What I Learned
- How to design state for **eventual consistency** without a central server
- Why **Lamport timestamps** are safer than wall clocks
- How to encode **merge laws** (commutative, associative, idempotent)
- Building a small but real **distributed data structure** with clear semantics

### ğŸ”¹ Future Improvements
- Per-character CRDT (RGA) for collaborative rich-text
- Delta syncs (state diffing) and Bloom filters
- Signatures over state for authenticated replication
- CRDT metrics dashboard (visualize merges & causality)


---

## ğŸ§  Day 14 â€” CRDT Collaborative Editor âœï¸

### ğŸ”¹ Project Title
**CRDT Collaborative Editor** â€” Logoot/LSEQ-style per-character CRDT with position identifiers, Lamport clocks, tombstones, and peer sync.

### ğŸ”¹ Project Description
A sequence-CRDT that lets multiple replicas edit a document **offline** and later **converge** without conflicts:
- Each character is tagged with a **position identifier** (list of `(digit, site)`), ordered lexicographically.
- **Concurrent inserts** choose random digits â€œbetweenâ€ neighbors (LSEQ/Logoot) â€” no global indices.
- **Deletes** are **tombstones** (idempotent).
- **Lamport clocks** ensure causal monotonicity; merge is **commutative, associative, idempotent**.
- Tiny **TCP sync**: peers exchange state JSON and both store the same merged document.

### ğŸ”¹ Concepts Used
- Sequence CRDTs (Logoot/LSEQ) & identifier allocation
- Causality via **Lamport clocks**
- Tombstones & idempotent merge
- Binary search insertion by identifier
- Minimal wire protocol over TCP

### ğŸ”¹ Example Sessions

Replica A

$ python Day-14/day14_crdt_editor.py init --site A $ python Day-14/day14_crdt_editor.py insert 0 "Hel" $ python Day-14/day14_crdt_editor.py insert 3 "lo" $ python Day-14/day14_crdt_editor.py show Hello

Replica B (offline)

$ python Day-14/day14_crdt_editor.py init --site B $ python Day-14/day14_crdt_editor.py insert 0 "H" $ python Day-14/day14_crdt_editor.py insert 1 "i!" $ python Day-14/day14_crdt_editor.py delete 1 --count 1   # remove 'i' $ python Day-14/day14_crdt_editor.py show H!

Sync

A acts as server:

$ python Day-14/day14_crdt_editor.py serve --port 9600

B pulls and merges:

$ python Day-14/day14_crdt_editor.py sync --host 127.0.0.1 --port 9600 ğŸ” Sync complete. States converged.

Both replicas now:

$ python Day-14/day14_crdt_editor.py show Hello!

### ğŸ”¹ What I Learned
- How sequence CRDTs avoid conflicts without locks or central servers  
- Why **identifier growth** and **random allocation** matter for balance  
- Designing merges to be **commutative/associative/idempotent**  
- Building a minimal yet correct **offline-first** collaborative system

### ğŸ”¹ Future Improvements
- Vector clocks for richer causality / causal broadcast
- Delta sync (op-based) + Bloom filters instead of full state
- Web UI (Flask + WebSockets) for real-time visual collaboration
- Balanced allocation strategies (boundary strategies, base tuning)


---

## ğŸ§  Day 15 â€” Neural Network From Scratch + Visualizer ğŸ§©

### ğŸ”¹ Project Title
**Neural Network From Scratch + Visualizer** â€” a NumPy implementation of a 2-layer feed-forward neural network with real-time loss and decision-boundary visualization.

### ğŸ”¹ Project Description
Implements the mathematical foundations of a small neural network *without* deep-learning frameworks:
- Forward / back-propagation equations derived and coded manually  
- Trains on synthetic 2-D data (circle classification)  
- Plots **loss curve** and **decision boundary** to verify learning visually  

### ğŸ”¹ Concepts Used
- Linear algebra (matrix multiplication, gradients)
- Activation functions (tanh, sigmoid)
- Back-propagation and gradient descent
- Overfitting control & visualization
- Scientific computing with NumPy + Matplotlib

### ğŸ”¹ Example Output

Epoch 1: loss=0.7039
Epoch 100: loss=0.3827
Epoch 500: loss=0.1204
Epoch 1000: loss=0.0558

*(Loss curve and 2-D classification boundary plots appear.)*

### ğŸ”¹ What I Learned
- How neural nets actually compute gradients and learn  
- Numerical stability issues ( Îµ to avoid log(0) )  
- Translating math (âˆ‚L/âˆ‚W, âˆ‚L/âˆ‚b) into vectorized NumPy code  
- Visual debugging of optimization processes  

### ğŸ”¹ Future Improvements
- Add ReLU, softmax, and multi-class classification  
- Mini-batch training and learning-rate schedules  
- Save/load weights and plot accuracy vs epochs  
- Build a Flask demo for interactive classification


---

## ğŸ§  Day 16 â€” MiniML: Mini functional language with Hindleyâ€“Milner type inference

### ğŸ”¹ Project Title
**MiniML** â€” a tiny ML-style functional language with parsing, evaluation, and Hindleyâ€“Milner (HM) type inference (let-polymorphism).

### ğŸ”¹ Project Description
This project implements a minimal functional language with:
- Integers and booleans
- Lambda expressions (anonymous functions), application, `let`, and `if`
- End-to-end pipeline: **lexer â†’ parser â†’ AST â†’ type inference (HM) â†’ interpreter**
- **Hindleyâ€“Milner (Algorithm W)** implementation: type variables, unification, occurs-check, generalization & instantiation
- A REPL with type queries and example programs

It demonstrates compiler/front-end and programming-language theory implemented in clear Python.

### ğŸ”¹ Concepts Used
- Lexing & parsing (recursive descent)
- AST design & interpreter (closures, lexical scope)
- Hindleyâ€“Milner type inference (unification, generalization)
- Let-polymorphism & type schemes
- Algorithmic thinking and formal methods

### ğŸ”¹ Example Usage / Output

MiniML> let id = \x -> x in id 5 Type: Int 5

MiniML> :t \f -> \x -> f (f x) Type: ((t0 -> t0) -> t0 -> t0) MiniML> let compose = \f -> \g -> \x -> f (g x) in compose Type: ((t0 -> t1) -> (t2 -> t0) -> t2 -> t1)

### ğŸ”¹ What I Learned
- How to implement a type checker and inference engine from first principles
- How unification and occurs-check prevent ill-typed terms
- How polymorphism emerges with `let` generalization
- How to structure a small compiler front-end and an interpreter

### ğŸ”¹ Future Improvements
- Add algebraic data types and pattern matching
- Improve parser to support multi-arg lambdas and syntactic sugar
- Add better pretty-printing of polymorphic types (rename type variables e.g., 'a, 'b)
- Compile to bytecode and add optimizer passes


---



ğŸŒŸ The Journey Ahead


Each project will advance in difficulty and creativity â€” reflecting both engineering skill and problem-solving ability that top universities like MIT, Stanford, and Harvard value deeply.


---

âœï¸ Author

ğŸ‘¨â€ğŸ’» Stuart Abhishek
16-year-old aspiring about Computer Science & AI
Dream Path:â†’ MIT / Stanford

> â€œDiscipline beats talent. Consistency builds brilliance.â€




---

ğŸªª License

This repository is licensed under the MIT License â€” free to learn, share, and contribute.


---

ğŸ’« Closing Note

> Every single day of this challenge represents my commitment to become a world-class programmer.
Through logic, creativity, and consistent hard work â€” Iâ€™ll reach the top, one line of code at a time.




---